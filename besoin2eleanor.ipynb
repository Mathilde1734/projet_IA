{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Besoin2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implémentation random forest from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 5.237086588143302e+20\n",
      "R^2 Score: -1.3730799368751908e+18\n",
      "                       Coefficient\n",
      "haut_tot                  0.227726\n",
      "haut_tronc                2.285528\n",
      "tronc_diam                0.167131\n",
      "fk_stadedev_Jeune        -8.396638\n",
      "fk_stadedev_senescent    29.282950\n",
      "...                            ...\n",
      "fk_nomtech_ULMJAP         4.392011\n",
      "fk_nomtech_ULMMIN        12.667372\n",
      "fk_nomtech_ULMRES        14.508682\n",
      "fk_nomtech_ULMRESreb      0.237516\n",
      "fk_nomtech_ULMRESsap      0.000000\n",
      "\n",
      "[225 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_prof = pd.read_csv('./Données/Data_Arbre.csv')\n",
    "data_clean = pd.read_csv(\"./Données/data_clean.csv\", encoding='utf-8', sep=\";\", decimal=\",\")\n",
    "\n",
    " # Sélectionner les colonnes pertinentes\n",
    "selected_columns = ['haut_tot', 'haut_tronc', 'tronc_diam', 'fk_stadedev', 'fk_nomtech']\n",
    "X = data_prof[selected_columns]\n",
    "y = data_prof['age_estim']\n",
    "X.shape\n",
    "\n",
    "# Sélectionner les colonnes catégorielles à encoder\n",
    "categorical_columns = ['fk_stadedev', 'fk_nomtech']\n",
    "\n",
    "# Appliquer l'encodage sur les colonnes catégorielles\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "X_encoded.shape\n",
    "\n",
    "# Normaliser les données\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#  Créer et entraîner le modèle de régression linéaire\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "X_train.shape\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "\n",
    "#  Afficher les coefficients du modèle\n",
    "coefficients = pd.DataFrame(model.coef_, columns=['Coefficient'], index=X_encoded.columns)\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Mean Squared Error: 76.82860361273896\n",
      "Random Forest - R^2 Score: 0.7985677486455527\n",
      "Random Forest - RMSE: 8.765192731066383\n"
     ]
    }
   ],
   "source": [
    "# Créer et entraîner le modèle de forêts aléatoires\n",
    "model_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred_rf = model_rf.predict(X_test_scaled)\n",
    "\n",
    "# Évaluer le modèle\n",
    "\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "\n",
    "print(f\"Random Forest - Mean Squared Error: {mse_rf}\")\n",
    "print(f\"Random Forest - R^2 Score: {r2_rf}\")\n",
    "print(f\"Random Forest - RMSE: {rmse_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART - Mean Squared Error: 97.84297209297588\n",
      "CART - R^2 Score: 0.743471451762393\n",
      "CART - RMSE: 9.891560650017563\n"
     ]
    }
   ],
   "source": [
    "# Créer et entraîner le modèle CART\n",
    "model_cart = DecisionTreeRegressor(max_depth=10, min_samples_split=2, random_state=42)\n",
    "model_cart.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred_cart = model_cart.predict(X_test_scaled)\n",
    "\n",
    "# Évaluer le modèle\n",
    "mse_cart = mean_squared_error(y_test, y_pred_cart)\n",
    "r2_cart = r2_score(y_test, y_pred_cart)\n",
    "rmse_cart = np.sqrt(mse_cart)\n",
    "\n",
    "print(f\"CART - Mean Squared Error: {mse_cart}\")\n",
    "print(f\"CART - R^2 Score: {r2_cart}\")\n",
    "print(f\"CART - RMSE: {rmse_cart}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression - Mean Squared Error: 16766.08356748525\n",
      "Regression - R^2 Score: 0.5668651854282613\n",
      "Regression - RMSE: 129.48391238870275\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#test CART\n",
    "\n",
    "\n",
    "# Définition de la fonction de variance pour la régression\n",
    "def variance(y):\n",
    "    return np.var(y)\n",
    "\n",
    "\n",
    "# Définition de la fonction de division des noeuds pour la régression\n",
    "def split(X, y, feature_index, threshold):\n",
    "    left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
    "    right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
    "    return X[left_indices], X[right_indices], y[left_indices], y[right_indices]\n",
    "\n",
    "\n",
    "# Définition de la classe Node pour l'arbre de régression\n",
    "class Node:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "# Définition de la classe DecisionTree pour l'arbre de régression\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=2, criterion='variance'):\n",
    "        self.root = None\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        if (depth >= self.max_depth or n_samples < self.min_samples_split or len(set(y)) == 1):\n",
    "            leaf_value = np.mean(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        best_feature, best_threshold = self._best_split(X, y, n_features)\n",
    "\n",
    "        left_X, right_X, left_y, right_y = split(X, y, best_feature, best_threshold)\n",
    "\n",
    "        left_child = self._grow_tree(left_X, left_y, depth + 1)\n",
    "        right_child = self._grow_tree(right_X, right_y, depth + 1)\n",
    "\n",
    "        return Node(best_feature, best_threshold, left_child, right_child)\n",
    "\n",
    "    def _best_split(self, X, y, n_features):\n",
    "        best_var = np.inf\n",
    "        split_idx, split_thresh = None, None\n",
    "\n",
    "        for feature_index in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                var = self._variance_index(X, y, feature_index, threshold)\n",
    "\n",
    "                if var < best_var:\n",
    "                    best_var = var\n",
    "                    split_idx = feature_index\n",
    "                    split_thresh = threshold\n",
    "\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _variance_index(self, X, y, feature_index, threshold):\n",
    "        left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
    "        right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
    "\n",
    "        if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "            return np.inf\n",
    "\n",
    "        left_var = variance(y[left_indices])\n",
    "        right_var = variance(y[right_indices])\n",
    "\n",
    "        n_left, n_right = len(left_indices), len(right_indices)\n",
    "        weighted_var = (n_left * left_var + n_right * right_var) / (n_left + n_right)\n",
    "\n",
    "        return weighted_var\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(inputs) for inputs in X])\n",
    "\n",
    "    def _predict(self, inputs):\n",
    "        node = self.root\n",
    "        while node.value is None:\n",
    "            if inputs[node.feature_index] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.value\n",
    "\n",
    "\n",
    "# Génération de données pour l'exemple de régression\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Création du modèle DecisionTree pour la régression\n",
    "model_regression = DecisionTree(min_samples_split=2, max_depth=10, criterion='variance')\n",
    "\n",
    "# Entraînement du modèle sur les données d'entraînement\n",
    "model_regression.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions sur les données de test\n",
    "y_pred_regression = model_regression.predict(X_test)\n",
    "\n",
    "# Évaluation du modèle\n",
    "mse_regression = mean_squared_error(y_test, y_pred_regression)\n",
    "r2_regression = r2_score(y_test, y_pred_regression)\n",
    "rmse_regression = np.sqrt(mse_regression)\n",
    "\n",
    "print(f\"Regression - Mean Squared Error: {mse_regression}\")\n",
    "print(f\"Regression - R^2 Score: {r2_regression}\")\n",
    "print(f\"Regression - RMSE: {rmse_regression}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Mean Squared Error: 0.1\n",
      "Random Forest - R^2 Score: 0.5980303487086724\n",
      "Random Forest - RMSE: 0.31622776601683794\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2, max_features=None):\n",
    "        self.n_estimators = n_estimators  # nombre d'arbres dans la forêt\n",
    "        self.max_depth = max_depth  # profondeur maximale des arbres\n",
    "        self.min_samples_split = min_samples_split  # nombre minimum d'échantillons requis pour diviser un nœud\n",
    "        self.max_features = max_features  # nombre de caractéristiques à considérer pour trouver la meilleure division\n",
    "        self.trees = []  # liste pour stocker les arbres\n",
    "        self.selected_features = []  # liste pour stocker les indices des caractéristiques sélectionnées pour chaque arbre\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape  # obtient le nombre d'échantillons et de caractéristiques\n",
    "        if not self.max_features:\n",
    "            self.max_features = int(np.sqrt(n_features))  # définit max_features à la racine carrée du nb carcateristique si non spécifié\n",
    "       \n",
    "        # construction de chaque arbre\n",
    "        for _ in range(self.n_estimators):\n",
    "            # bootstrap sampling\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)  # sélectionne des échantillons aléatoires avec remise\n",
    "            X_bootstrap = X[indices]  # on crée l'échantillon bootstrap pour les caractéristiques\n",
    "            y_bootstrap = y[indices]  # on crée l'échantillon bootstrap pour les étiquettes\n",
    "\n",
    "            # sélection aléatoire des caractéristques\n",
    "            selected_features = np.random.choice(n_features, self.max_features, replace=False)  # sélectionne aléatoirement des caractéristiques \n",
    "            self.selected_features.append(selected_features)  # sauvegarde les indices des caractéristiques sélectionnées\n",
    "\n",
    "            # création et entraînement de l'arbre de décision avec scikit-learn\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth, min_samples_split=self.min_samples_split)  # crée un arbre de décision\n",
    "            tree.fit(X_bootstrap[:, selected_features], y_bootstrap)  # entraîne l'arbre sur l'échantillon bootstrap et les caractéristiques sélectionnées\n",
    "\n",
    "            self.trees.append(tree)  # ajoute l'arbre à la forêt\n",
    "\n",
    "    def predict(self, X):\n",
    "        # agréger les prédictions de chaque arbre\n",
    "        predictions = np.zeros((X.shape[0], len(self.trees)))  # crée un tableau pour stocker les prédictions de chaque arbre pour chaque échantillon\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            selected_features = self.selected_features[i]  # récupère les caractéristiques sélectionnées pour cet arbre\n",
    "            predictions[:, i] = tree.predict(X[:, selected_features])  # prédictions de l'arbre en utilisant les caractéristiques sélectionnées\n",
    "        \n",
    "        # vote majoritaire\n",
    "        return np.round(np.mean(predictions, axis=1)).astype(int)  # calcule la moyenne des prédictions pour chaque échantillon et arrondit pour obtenir les classes finales\n",
    "\n",
    "# Exemple d'utilisation\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)  # Génère un ensemble de données de classification avec 1000 échantillons et 20 caractéristiques\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Divise les données en ensembles d'entraînement et de test (80% - 20%)\n",
    "\n",
    "# Création du modèle de Random Forest\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2, max_features=None)  # Crée une instance de RandomForestClassifier avec 100 arbres\n",
    "\n",
    "# entraînement du modèle sur les données d'entraînement\n",
    "rf_classifier.fit(X_train, y_train)  # Entraîne le modèle sur les données d'entraînement\n",
    "\n",
    "# prédictions sur les données de test\n",
    "y_pred = rf_classifier.predict(X_test)  # Utilise le modèle pour prédire les classes des échantillons de test\n",
    "\n",
    "# Évaluation supplémentaire avec MSE, R2 Score et RMSE\n",
    "mse_rf = mean_squared_error(y_test, y_pred)\n",
    "r2_rf = r2_score(y_test, y_pred)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "\n",
    "print(f\"Random Forest - Mean Squared Error: {mse_rf}\")\n",
    "print(f\"Random Forest - R^2 Score: {r2_rf}\")\n",
    "print(f\"Random Forest - RMSE: {rmse_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART v1 classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "#calcul des critères de division\n",
    "\n",
    "# Définition de la fonction gini pour la classifiction pour un ensemble de labels y\n",
    "def gini(y):\n",
    "    m = len(y)\n",
    "    return 1.0 - sum((np.sum(y == c) / m) ** 2 for c in np.unique(y))\n",
    "\n",
    "#ou variance pour la régression\n",
    "def variance(y):\n",
    "    return np.var(y)\n",
    "\n",
    "# Définition de la fonction split de divisions des noeuds\n",
    "#fonction pour diviser les données basées sur une caractéristique et un seuil\n",
    "def split(X, y, feature_index, threshold):\n",
    "    left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
    "    right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
    "    return X[left_indices], X[right_indices], y[left_indices], y[right_indices]\n",
    "\n",
    "#Construction de l'arbre de décision \n",
    "\n",
    "# Définition de la classe Node: noeud de l'abre de décision\n",
    "class Node:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index  #index de la caractéristique utilisée pour la division\n",
    "        self.threshold = threshold          #seuil de division\n",
    "        self.left = left                    #sous-arbre gauche\n",
    "        self.right = right                  #sous-arbre droit\n",
    "        self.value = value                  #valeur de la feuille (si c'est une feuille)\n",
    "\n",
    "\n",
    "# Définition de la classe DecisionTree responsable de la construction et de l'utilisation de l'arbre de décision\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=2, criterion='gini'): \n",
    "        self.root = None #racine de l'abre\n",
    "        self.min_samples_split = min_samples_split #nombre minimum d'échantillons pour diviser un nœud\n",
    "        self.max_depth = max_depth #profondeur maximale de l'arbre\n",
    "        self.criterion = criterion #critère de division (ici nous utilisons Gini)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.n_classes = len(set(y)) #nombre de classes uniques \n",
    "        self.root = self._grow_tree(X, y)#construction de l'arbre\n",
    "    \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        # vérifie plusieurs conditions pour déterminer s'il faut arrêter la croissance de l'arbre et créer une feuille\n",
    "        n_samples, n_features = X.shape #initialisation\n",
    "        #critère d'arret\n",
    "        if (depth >= self.max_depth or n_samples < self.min_samples_split or len(set(y)) == 1):\n",
    "            #critères de profondeur maximale, de nombre minimumd'echantillons, appartenance des classes\n",
    "            leaf_value = self._most_common_label(y) #si l'un des critères est satisfait, une feuille est créé\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        # trouver la meilleure division (best split)\n",
    "        best_feature, best_threshold = self._best_split(X, y, n_features)\n",
    "        \n",
    "        #division des données\n",
    "        left_X, right_X, left_y, right_y = split(X, y, best_feature, best_threshold)\n",
    "       \n",
    "        #construire les sous arbres par récursivité \n",
    "        left_child = self._grow_tree(left_X, left_y, depth + 1)\n",
    "        right_child = self._grow_tree(right_X, right_y, depth + 1)\n",
    "        return Node(best_feature, best_threshold, left_child, right_child)\n",
    "    \n",
    "    #trouve la meilleure caractéristique et le meilleur seuil pour diviser les données\n",
    "    def _best_split(self, X, y, n_features):\n",
    "        #initialisation des variables\n",
    "        best_gini = 1.0 #initialisation à 1 soit la pire valeur possible pour l'impureté de Gini\n",
    "        split_idx, split_thresh = None, None # stockeront l'index de la caractéristique et  le seuil correspondant à la meilleure division\n",
    "        \n",
    "        \n",
    "        for feature_index in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_index]) #tableau des valeurs uniques dans la colonne feature_index de X\n",
    "            for threshold in thresholds: #boucle parcourt chaque valeur unique (seuil) pour la caractéristique courante\n",
    "                gini = self._gini_index(X, y, feature_index, threshold)\n",
    "                #mise à jour de la meilleure division pour les 3 variables\n",
    "                if gini < best_gini: #Si l'impureté de Gini pour la division courante est meilleure que best_gini, on met à jour best_gini, split_idx et split_thresh\n",
    "                    best_gini = gini\n",
    "                    split_idx = feature_index\n",
    "                    split_thresh = threshold\n",
    "        return split_idx, split_thresh\n",
    "    \n",
    "    def _gini_index(self, X, y, feature_index, threshold): #calcule l'impureté de Gini pour une division donnée par feature_index et threshold\n",
    "       #on divise les indices \n",
    "        left_indices = np.where(X[:, feature_index] <= threshold)[0] #indices des échantillons pour lesquels la valeur de la caractéristique est inférieure ou égale au seuil\n",
    "        right_indices = np.where(X[:, feature_index] > threshold)[0] # indices des échantillons pour lesquels la valeur de la caractéristique est supérieure au seuil\n",
    "        #vérification des divisions vides\n",
    "        if len(left_indices) == 0 or len(right_indices) == 0:#si une des divisions (gauche ou droite) est vide-->\n",
    "            return 1.0  #retourner 1.0 =la pire impureté de Gini (division inutile)\n",
    "       \n",
    "        #on calcule de l'Impureté de Gini pour chaque division\n",
    "        left_gini = gini(y[left_indices]) # impureté de Gini pour les labels des échantillons dans la division gauche\n",
    "        right_gini = gini(y[right_indices])# \"\" droite\n",
    "        #calcul des poids pour les divisions\n",
    "        n_left, n_right = len(left_indices), len(right_indices)\n",
    "        #calcul de l'impureté de Gini pondérée par le nombre d'echantillons dans chaque division\n",
    "        weighted_gini = (n_left * left_gini + n_right * right_gini) / (n_left + n_right)\n",
    "        \n",
    "        return weighted_gini\n",
    "    \n",
    "    def _most_common_label(self, y):\n",
    "        return Counter(y).most_common(1)[0][0] #retourne le label le plus fréquent dans un ensemble de labels y\n",
    "    \n",
    "    def predict(self, X):#utilise l'arbre de décision pour faire des prédictions sur un ensmeble de nouvelles données X\n",
    "        return np.array([self._predict(inputs) for inputs in X]) #applique la fonction _predict sur chaque exemple d'entrée dans X et retourne un tableau des prédictions\n",
    "    \n",
    "    def _predict(self, inputs):# traverse l'arbre de décision pour prédire la classe pour un exemple d'entrée donné\n",
    "        node = self.root #initialisation\n",
    "        #on traverse l'arbre\n",
    "        while node.value is None: #Tant que le nœud courant n'est pas une feuille (n'a pas de valeur de classe), la fonction vérifie la valeur de la caractéristique de inputs correspondant à node.feature_index\n",
    "            if inputs[node.feature_index] <= node.threshold: #si cette valeur est inférieure ou égale à node.threshold elle se déplace vers le sous-arbre gauche (node.left), sinon elle se déplace vers le sous-arbre droit (node.right).\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.value #retourne la prédiction\n",
    "\n",
    "# test avec de données random pour l'exemple\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#on évalue des performances\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART v2 regression commenter!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r2_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 107\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Évaluer le modèle\u001b[39;00m\n\u001b[0;32m    106\u001b[0m mse_cart \u001b[38;5;241m=\u001b[39m mean_squared_error(y_test, y_pred)\n\u001b[1;32m--> 107\u001b[0m r2_cart \u001b[38;5;241m=\u001b[39m \u001b[43mr2_score\u001b[49m(y_test, y_pred)\n\u001b[0;32m    108\u001b[0m rmse_cart \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mse_cart)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCART - Mean Squared Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmse_cart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'r2_score' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Définition de la fonction de variance pour la régression\n",
    "def variance(y):\n",
    "    return np.var(y)\n",
    "\n",
    "# Définition de la fonction de division des noeuds pour la régression\n",
    "def split(X, y, feature_index, threshold):\n",
    "    left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
    "    right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
    "    return X[left_indices], X[right_indices], y[left_indices], y[right_indices]\n",
    "\n",
    "# Définition de la classe Node pour l'arbre de régression\n",
    "class Node:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "# Définition de la classe DecisionTree pour l'arbre de régression\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=2, criterion='variance'):\n",
    "        self.root = None\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        if (depth >= self.max_depth or n_samples < self.min_samples_split or len(set(y)) == 1):\n",
    "            leaf_value = np.mean(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        best_feature, best_threshold = self._best_split(X, y, n_features)\n",
    "\n",
    "        left_X, right_X, left_y, right_y = split(X, y, best_feature, best_threshold)\n",
    "\n",
    "        left_child = self._grow_tree(left_X, left_y, depth + 1)\n",
    "        right_child = self._grow_tree(right_X, right_y, depth + 1)\n",
    "\n",
    "        return Node(best_feature, best_threshold, left_child, right_child)\n",
    "\n",
    "    def _best_split(self, X, y, n_features):\n",
    "        best_var = np.inf\n",
    "        split_idx, split_thresh = None, None\n",
    "\n",
    "        for feature_index in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                var = self._variance_index(X, y, feature_index, threshold)\n",
    "\n",
    "                if var < best_var:\n",
    "                    best_var = var\n",
    "                    split_idx = feature_index\n",
    "                    split_thresh = threshold\n",
    "\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _variance_index(self, X, y, feature_index, threshold):\n",
    "        left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
    "        right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
    "\n",
    "        if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "            return np.inf\n",
    "\n",
    "        left_var = variance(y[left_indices])\n",
    "        right_var = variance(y[right_indices])\n",
    "\n",
    "        n_left, n_right = len(left_indices), len(right_indices)\n",
    "        weighted_var = (n_left * left_var + n_right * right_var) / (n_left + n_right)\n",
    "\n",
    "        return weighted_var\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(inputs) for inputs in X])\n",
    "\n",
    "    def _predict(self, inputs):\n",
    "        node = self.root\n",
    "        while node.value is None:\n",
    "            if inputs[node.feature_index] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r2_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Évaluer le modèle\u001b[39;00m\n\u001b[0;32m     11\u001b[0m mse_cart \u001b[38;5;241m=\u001b[39m mean_squared_error(y_test, y_pred)\n\u001b[1;32m---> 12\u001b[0m r2_cart \u001b[38;5;241m=\u001b[39m \u001b[43mr2_score\u001b[49m(y_test, y_pred)\n\u001b[0;32m     13\u001b[0m rmse_cart \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mse_cart)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCART - Mean Squared Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmse_cart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'r2_score' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART BON Emilie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2, max_features=None):\n",
    "        self.n_estimators = n_estimators  # nombre d'arbres dans la forêt\n",
    "        self.max_depth = max_depth  #profondeur maximale des arbres\n",
    "        self.min_samples_split = min_samples_split  #nombre minimum d'échantillons requis pour diviser un nœud\n",
    "        self.max_features = max_features  # nombre de caractéristiques à considérer pour trouver la meilleure division\n",
    "        self.trees = []  # liste pour stocker les arbres\n",
    "        self.selected_features = []  # liste pour stocker les indices des caractéristiques sélectionnées pour chaque arbre\n",
    "\n",
    "    def fit(self, X, y): #méthode fit\n",
    "        n_samples, n_features = X.shape  #obtient le nombre d'échantillons et de caractéristiques\n",
    "        if not self.max_features:\n",
    "            self.max_features = int(np.sqrt(n_features))  # définit max_features à la racine carrée du nb carcateristique si non spécifié\n",
    "       \n",
    "        #construction de chaque arbre\n",
    "        for _ in range(self.n_estimators):\n",
    "            #bootstrap sampling\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)  #sélectionne des échantillons aléatoires avec remise\n",
    "            X_bootstrap = X[indices]  #on crée l'échantillon bootstrap pour les caractéristiques\n",
    "            y_bootstrap = y[indices]  #on crée l'échantillon bootstrap pour les étiquettes\n",
    "\n",
    "            # sélection aléatoire des caractéristques\n",
    "            selected_features = np.random.choice(n_features, self.max_features, replace=False)  #sélectionne aléatoirement des caractéristiques \n",
    "            self.selected_features.append(selected_features)  #sauvegarde les indices des caractéristiques sélectionnées\n",
    "\n",
    "            # création et entraînement de l'arbre de décision avec scikit-learn\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth, min_samples_split=self.min_samples_split)  #crée un arbre de décision\n",
    "            tree.fit(X_bootstrap[:, selected_features], y_bootstrap)  #entraîne l'arbre sur l'échantillon bootstrap et les caractéristiques sélectionnées\n",
    "\n",
    "            self.trees.append(tree)  # ajoute l'arbre à la forêt\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        # agréger les prédictions de chaque arbre\n",
    "        predictions = np.zeros((X.shape[0], len(self.trees)))  #crée un tableau pour stocker les prédictions de chaque arbre pour chaque échantillon\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            selected_features = self.selected_features[i]  #récupère les caractéristiques sélectionnées pour cet arbre\n",
    "            predictions[:, i] = tree.predict(X[:, selected_features])  #prédictions de l'arbre en utilisant les caractéristiques sélectionnées\n",
    "        \n",
    "        # vote majoritaire\n",
    "        return np.round(np.mean(predictions, axis=1)).astype(int)  #calcule la moyenne des prédictions pour chaque échantillon et arrondit pour obtenir les classes finales\n",
    "\n",
    "# Exemple d'utilisation\n",
    "\n",
    "# test données random\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)  # Génère un ensemble de données de classification avec 1000 échantillons et 20 caractéristiques\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Divise les données en ensembles d'entraînement et de test (80% - 20%)\n",
    "\n",
    "# Création du modèle de Random Forest\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2, max_features=None)  # Crée une instance de RandomForestClassifier avec 100 arbres\n",
    "\n",
    "#entraînement du modèle sur les données d'entraînement\n",
    "rf_classifier.fit(X_train, y_train)  # Entraîne le modèle sur les données d'entraînement\n",
    "\n",
    "#prédictions sur les données de test\n",
    "y_pred = rf_classifier.predict(X_test)  # Utilise le modèle pour prédire les classes des échantillons de test\n",
    "\n",
    "#évaluation des performances\n",
    "accuracy = accuracy_score(y_test, y_pred)  # Évalue la performance en calculant l'exactitude des prédictions\n",
    "print(f\"Accuracy: {accuracy}\")  # Affiche l'exactitude des prédictions\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
