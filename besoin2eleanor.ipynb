{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Besoin2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implémentation random forest from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "#calcul des critères de division\n",
    "\n",
    "# Définition de la fonction gini pour la classifiction pour un ensemble de labels y\n",
    "def gini(y):\n",
    "    m = len(y)\n",
    "    return 1.0 - sum((np.sum(y == c) / m) ** 2 for c in np.unique(y))\n",
    "\n",
    "#ou variance pour la régression\n",
    "def variance(y):\n",
    "    return np.var(y)\n",
    "\n",
    "# Définition de la fonction split de divisions des noeuds\n",
    "#fonction pour diviser les données basées sur une caractéristique et un seuil\n",
    "def split(X, y, feature_index, threshold):\n",
    "    left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
    "    right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
    "    return X[left_indices], X[right_indices], y[left_indices], y[right_indices]\n",
    "\n",
    "#Construction de l'arbre de décision \n",
    "\n",
    "# Définition de la classe Node: noeud de l'abre de décision\n",
    "class Node:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index  #index de la caractéristique utilisée pour la division\n",
    "        self.threshold = threshold          #seuil de division\n",
    "        self.left = left                    #sous-arbre gauche\n",
    "        self.right = right                  #sous-arbre droit\n",
    "        self.value = value                  #valeur de la feuille (si c'est une feuille)\n",
    "\n",
    "\n",
    "# Définition de la classe DecisionTree responsable de la construction et de l'utilisation de l'arbre de décision\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=2, criterion='gini'): \n",
    "        self.root = None #racine de l'abre\n",
    "        self.min_samples_split = min_samples_split #nombre minimum d'échantillons pour diviser un nœud\n",
    "        self.max_depth = max_depth #profondeur maximale de l'arbre\n",
    "        self.criterion = criterion #critère de division (ici nous utilisons Gini)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.n_classes = len(set(y)) #nombre de classes uniques \n",
    "        self.root = self._grow_tree(X, y)#construction de l'arbre\n",
    "    \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        # vérifie plusieurs conditions pour déterminer s'il faut arrêter la croissance de l'arbre et créer une feuille\n",
    "        n_samples, n_features = X.shape #initialisation\n",
    "        #critère d'arret\n",
    "        if (depth >= self.max_depth or n_samples < self.min_samples_split or len(set(y)) == 1):\n",
    "            #critères de profondeur maximale, de nombre minimumd'echantillons, appartenance des classes\n",
    "            leaf_value = self._most_common_label(y) #si l'un des critères est satisfait, une feuille est créé\n",
    "            return Node(value=leaf_value)\n",
    "        \n",
    "        # trouver la meilleure division (best split)\n",
    "        best_feature, best_threshold = self._best_split(X, y, n_features)\n",
    "        \n",
    "        #division des données\n",
    "        left_X, right_X, left_y, right_y = split(X, y, best_feature, best_threshold)\n",
    "       \n",
    "        #construire les sous arbres par récursivité \n",
    "        left_child = self._grow_tree(left_X, left_y, depth + 1)\n",
    "        right_child = self._grow_tree(right_X, right_y, depth + 1)\n",
    "        return Node(best_feature, best_threshold, left_child, right_child)\n",
    "    \n",
    "    #trouve la meilleure caractéristique et le meilleur seuil pour diviser les données\n",
    "    def _best_split(self, X, y, n_features):\n",
    "        #initialisation des variables\n",
    "        best_gini = 1.0 #initialisation à 1 soit la pire valeur possible pour l'impureté de Gini\n",
    "        split_idx, split_thresh = None, None # stockeront l'index de la caractéristique et  le seuil correspondant à la meilleure division\n",
    "        \n",
    "        \n",
    "        for feature_index in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_index]) #tableau des valeurs uniques dans la colonne feature_index de X\n",
    "            for threshold in thresholds: #boucle parcourt chaque valeur unique (seuil) pour la caractéristique courante\n",
    "                gini = self._gini_index(X, y, feature_index, threshold)\n",
    "                #mise à jour de la meilleure division pour les 3 variables\n",
    "                if gini < best_gini: #Si l'impureté de Gini pour la division courante est meilleure que best_gini, on met à jour best_gini, split_idx et split_thresh\n",
    "                    best_gini = gini\n",
    "                    split_idx = feature_index\n",
    "                    split_thresh = threshold\n",
    "        return split_idx, split_thresh\n",
    "    \n",
    "    def _gini_index(self, X, y, feature_index, threshold): #calcule l'impureté de Gini pour une division donnée par feature_index et threshold\n",
    "       #on divise les indices \n",
    "        left_indices = np.where(X[:, feature_index] <= threshold)[0] #indices des échantillons pour lesquels la valeur de la caractéristique est inférieure ou égale au seuil\n",
    "        right_indices = np.where(X[:, feature_index] > threshold)[0] # indices des échantillons pour lesquels la valeur de la caractéristique est supérieure au seuil\n",
    "        #vérification des divisions vides\n",
    "        if len(left_indices) == 0 or len(right_indices) == 0:#si une des divisions (gauche ou droite) est vide-->\n",
    "            return 1.0  #retourner 1.0 =la pire impureté de Gini (division inutile)\n",
    "       \n",
    "        #on calcule de l'Impureté de Gini pour chaque division\n",
    "        left_gini = gini(y[left_indices]) # impureté de Gini pour les labels des échantillons dans la division gauche\n",
    "        right_gini = gini(y[right_indices])# \"\" droite\n",
    "        #calcul des poids pour les divisions\n",
    "        n_left, n_right = len(left_indices), len(right_indices)\n",
    "        #calcul de l'impureté de Gini pondérée par le nombre d'echantillons dans chaque division\n",
    "        weighted_gini = (n_left * left_gini + n_right * right_gini) / (n_left + n_right)\n",
    "        \n",
    "        return weighted_gini\n",
    "    \n",
    "    def _most_common_label(self, y):\n",
    "        return Counter(y).most_common(1)[0][0] #retourne le label le plus fréquent dans un ensemble de labels y\n",
    "    \n",
    "    def predict(self, X):#utilise l'arbre de décision pour faire des prédictions sur un ensmeble de nouvelles données X\n",
    "        return np.array([self._predict(inputs) for inputs in X]) #applique la fonction _predict sur chaque exemple d'entrée dans X et retourne un tableau des prédictions\n",
    "    \n",
    "    def _predict(self, inputs):# traverse l'arbre de décision pour prédire la classe pour un exemple d'entrée donné\n",
    "        node = self.root #initialisation\n",
    "        #on traverse l'arbre\n",
    "        while node.value is None: #Tant que le nœud courant n'est pas une feuille (n'a pas de valeur de classe), la fonction vérifie la valeur de la caractéristique de inputs correspondant à node.feature_index\n",
    "            if inputs[node.feature_index] <= node.threshold: #si cette valeur est inférieure ou égale à node.threshold elle se déplace vers le sous-arbre gauche (node.left), sinon elle se déplace vers le sous-arbre droit (node.right).\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.value #retourne la prédiction\n",
    "\n",
    "# test avec de données random pour l'exemple\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#on évalue des performances\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2, max_features=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "        self.selected_features = []  # Ajouter un attribut pour stocker les caractéristiques sélectionnées\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        if not self.max_features:\n",
    "            self.max_features = int(np.sqrt(n_features))\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Bootstrap sampling\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_bootstrap = X[indices]\n",
    "            y_bootstrap = y[indices]\n",
    "\n",
    "            # Randomly select features\n",
    "            selected_features = np.random.choice(n_features, self.max_features, replace=False)\n",
    "            self.selected_features.append(selected_features)  # Sauvegarder les caractéristiques sélectionnées\n",
    "\n",
    "            # Create decision tree using DecisionTreeClassifier from scikit-learn\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X_bootstrap[:, selected_features], y_bootstrap)\n",
    "\n",
    "            # Add tree to forest\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Aggregate predictions from each tree\n",
    "        predictions = np.zeros((X.shape[0], len(self.trees)))\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            selected_features = self.selected_features[i]  # Récupérer les caractéristiques sélectionnées pour cet arbre\n",
    "            predictions[:, i] = tree.predict(X[:, selected_features])  \n",
    "        \n",
    "        # For classification, use majority vote\n",
    "        return np.round(np.mean(predictions, axis=1)).astype(int)\n",
    "\n",
    "# Exemple d'utilisation\n",
    "\n",
    "# Génération de données synthétiques pour l'exemple\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Création du modèle de Random Forest\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2, max_features=None)\n",
    "\n",
    "# Entraînement du modèle sur les données d'entraînement\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions sur les données de test\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Évaluation des performances\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
