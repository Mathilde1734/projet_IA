{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Besoin2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implémentation random forest from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 5.237086588143302e+20\n",
      "R^2 Score: -1.3730799368751908e+18\n",
      "                       Coefficient\n",
      "haut_tot                  0.227726\n",
      "haut_tronc                2.285528\n",
      "tronc_diam                0.167131\n",
      "fk_stadedev_Jeune        -8.396638\n",
      "fk_stadedev_senescent    29.282950\n",
      "...                            ...\n",
      "fk_nomtech_ULMJAP         4.392011\n",
      "fk_nomtech_ULMMIN        12.667372\n",
      "fk_nomtech_ULMRES        14.508682\n",
      "fk_nomtech_ULMRESreb      0.237516\n",
      "fk_nomtech_ULMRESsap      0.000000\n",
      "\n",
      "[225 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_prof = pd.read_csv('./Données/Data_Arbre.csv')\n",
    "data_clean = pd.read_csv(\"./Données/data_clean.csv\", encoding='utf-8', sep=\";\", decimal=\",\")\n",
    "\n",
    " # Sélectionner les colonnes pertinentes\n",
    "selected_columns = ['haut_tot', 'haut_tronc', 'tronc_diam', 'fk_stadedev', 'fk_nomtech']\n",
    "X = data_prof[selected_columns]\n",
    "y = data_prof['age_estim']\n",
    "X.shape\n",
    "\n",
    "# Sélectionner les colonnes catégorielles à encoder\n",
    "categorical_columns = ['fk_stadedev', 'fk_nomtech']\n",
    "\n",
    "# Appliquer l'encodage sur les colonnes catégorielles\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "X_encoded.shape\n",
    "\n",
    "# Normaliser les données\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#  Créer et entraîner le modèle de régression linéaire\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "X_train.shape\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "\n",
    "#  Afficher les coefficients du modèle\n",
    "coefficients = pd.DataFrame(model.coef_, columns=['Coefficient'], index=X_encoded.columns)\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Mean Squared Error: 76.82860361273896\n",
      "Random Forest - R^2 Score: 0.7985677486455527\n",
      "Random Forest - RMSE: 8.765192731066383\n"
     ]
    }
   ],
   "source": [
    "# Créer et entraîner le modèle de forêts aléatoires\n",
    "model_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred_rf = model_rf.predict(X_test_scaled)\n",
    "\n",
    "# Évaluer le modèle\n",
    "\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "\n",
    "print(f\"Random Forest - Mean Squared Error: {mse_rf}\")\n",
    "print(f\"Random Forest - R^2 Score: {r2_rf}\")\n",
    "print(f\"Random Forest - RMSE: {rmse_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART - Mean Squared Error: 97.84297209297588\n",
      "CART - R^2 Score: 0.743471451762393\n",
      "CART - RMSE: 9.891560650017563\n"
     ]
    }
   ],
   "source": [
    "# Créer et entraîner le modèle CART\n",
    "model_cart = DecisionTreeRegressor(max_depth=10, min_samples_split=2, random_state=42)\n",
    "model_cart.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred_cart = model_cart.predict(X_test_scaled)\n",
    "\n",
    "# Évaluer le modèle\n",
    "mse_cart = mean_squared_error(y_test, y_pred_cart)\n",
    "r2_cart = r2_score(y_test, y_pred_cart)\n",
    "rmse_cart = np.sqrt(mse_cart)\n",
    "\n",
    "print(f\"CART - Mean Squared Error: {mse_cart}\")\n",
    "print(f\"CART - R^2 Score: {r2_cart}\")\n",
    "print(f\"CART - RMSE: {rmse_cart}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression - Mean Squared Error: 16766.08356748525\n",
      "Regression - R^2 Score: 0.5668651854282613\n",
      "Regression - RMSE: 129.48391238870275\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Définition de la fonction de variance pour la régression\n",
    "def variance(y):\n",
    "    return np.var(y)  # Calcule et retourne la variance de y\n",
    "\n",
    "# Définition de la fonction de division des noeuds pour la régression\n",
    "def split(X, y, feature_index, threshold):\n",
    "    # Divise les indices en deux groupes : ceux où la caractéristique est <= seuil et ceux où elle est > seuil\n",
    "    left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
    "    right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
    "    return X[left_indices], X[right_indices], y[left_indices], y[right_indices]\n",
    "\n",
    "# Définition de la classe Node pour l'arbre de régression\n",
    "class Node:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index  # Index de la caractéristique utilisée pour la division\n",
    "        self.threshold = threshold  # Seuil utilisé pour la division\n",
    "        self.left = left  # Enfant gauche\n",
    "        self.right = right  # Enfant droit\n",
    "        self.value = value  # Valeur de la feuille si c'est une feuille\n",
    "\n",
    "# Définition de la classe DecisionTree pour l'arbre de régression\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=None, criterion='variance'):\n",
    "        self.root = None  # Racine de l'arbre\n",
    "        self.min_samples_split = min_samples_split  # Nombre minimum d'échantillons pour diviser un nœud\n",
    "        self.max_depth = max_depth  # Profondeur maximale de l'arbre\n",
    "        self.criterion = criterion  # Critère utilisé pour la division (ici, la variance)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._grow_tree(X, y)  # Construit l'arbre en appelant _grow_tree\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape  # Nombre d'échantillons et de caractéristiques\n",
    "        # Critères pour créer une feuille : profondeur maximale atteinte, trop peu d'échantillons, ou variance nulle\n",
    "        if (depth >= self.max_depth or n_samples < self.min_samples_split or len(set(y)) == 1):\n",
    "            leaf_value = np.mean(y)  # Valeur de la feuille : moyenne des valeurs de y\n",
    "            return Node(value=leaf_value)  # Retourne un nœud feuille\n",
    "\n",
    "        best_feature, best_threshold = self._best_split(X, y, n_features)  # Trouve la meilleure division\n",
    "\n",
    "        if best_feature is None:  # Si aucune division valide n'est trouvée\n",
    "            return Node(value=np.mean(y))  # Retourne un nœud feuille\n",
    "\n",
    "        # Divise les données selon la meilleure division trouvée\n",
    "        left_X, right_X, left_y, right_y = split(X, y, best_feature, best_threshold)\n",
    "\n",
    "        # Construit les sous-arbres récursivement\n",
    "        left_child = self._grow_tree(left_X, left_y, depth + 1)\n",
    "        right_child = self._grow_tree(right_X, right_y, depth + 1)\n",
    "\n",
    "        # Retourne un nœud interne avec la meilleure division et les sous-arbres construits\n",
    "        return Node(best_feature, best_threshold, left_child, right_child)\n",
    "\n",
    "    def _best_split(self, X, y, n_features):\n",
    "        best_var = np.inf  # Initialise la meilleure variance à l'infini\n",
    "        split_idx, split_thresh = None, None  # Initialise les meilleurs index et seuil de division\n",
    "\n",
    "        for feature_index in range(n_features):  # Parcourt toutes les caractéristiques\n",
    "            thresholds = np.unique(X[:, feature_index])  # Obtient les seuils uniques pour cette caractéristique\n",
    "\n",
    "            for threshold in thresholds:  # Parcourt tous les seuils possibles\n",
    "                var = self._variance_index(X, y, feature_index, threshold)  # Calcule la variance pour cette division\n",
    "\n",
    "                if var < best_var:  # Si cette division réduit la variance\n",
    "                    best_var = var  # Met à jour la meilleure variance\n",
    "                    split_idx = feature_index  # Met à jour le meilleur index de caractéristique\n",
    "                    split_thresh = threshold  # Met à jour le meilleur seuil\n",
    "\n",
    "        return split_idx, split_thresh  # Retourne le meilleur index de caractéristique et le meilleur seuil\n",
    "\n",
    "    def _variance_index(self, X, y, feature_index, threshold):\n",
    "        left_indices = np.where(X[:, feature_index] <= threshold)[0]  # Indices des échantillons dans le groupe de gauche\n",
    "        right_indices = np.where(X[:, feature_index] > threshold)[0]  # Indices des échantillons dans le groupe de droite\n",
    "\n",
    "        if len(left_indices) == 0 or len(right_indices) == 0:  # Si une division est vide, retourne une variance infinie\n",
    "            return np.inf\n",
    "\n",
    "        # Calcule les variances des groupes de gauche et de droite\n",
    "        left_var = variance(y[left_indices])\n",
    "        right_var = variance(y[right_indices])\n",
    "\n",
    "        n_left, n_right = len(left_indices), len(right_indices)  # Nombre d'échantillons dans les groupes de gauche et de droite\n",
    "        weighted_var = (n_left * left_var + n_right * right_var) / (n_left + n_right)  # Variance pondérée\n",
    "\n",
    "        return weighted_var  # Retourne la variance pondérée\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(inputs) for inputs in X])  # Prédit pour chaque échantillon dans X\n",
    "\n",
    "    def _predict(self, inputs):\n",
    "        node = self.root  # Commence à la racine\n",
    "        while node.value is None:  # Tant que le nœud n'est pas une feuille\n",
    "            if inputs[node.feature_index] <= node.threshold:  # Si la caractéristique est <= seuil\n",
    "                node = node.left  # Va à gauche\n",
    "            else:  # Sinon\n",
    "                node = node.right  # Va à droite\n",
    "        return node.value  # Retourne la valeur de la feuille\n",
    "\n",
    "# Exemple de régression\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)  # Génère un jeu de données de régression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Divise les données en train/test\n",
    "\n",
    "# Création du modèle DecisionTree pour la régression\n",
    "model_regression = DecisionTree(min_samples_split=2, max_depth=10, criterion='variance')\n",
    "\n",
    "# Entraînement du modèle sur les données d'entraînement\n",
    "model_regression.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions sur les données de test\n",
    "y_pred_regression = model_regression.predict(X_test)\n",
    "\n",
    "# Évaluation du modèle\n",
    "mse_regression = mean_squared_error(y_test, y_pred_regression)  # Calcule l'erreur quadratique moyenne\n",
    "r2_regression = r2_score(y_test, y_pred_regression)  # Calcule le coefficient de détermination\n",
    "rmse_regression = np.sqrt(mse_regression)  # Calcule la racine de l'erreur quadratique moyenne\n",
    "\n",
    "# Affiche les résultats de l'évaluation\n",
    "print(f\"Regression - Mean Squared Error: {mse_regression}\")\n",
    "print(f\"Regression - R^2 Score: {r2_regression}\")\n",
    "print(f\"Regression - RMSE: {rmse_regression}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Mean Squared Error: 0.145\n",
      "Random Forest - R^2 Score: 0.41714400562757503\n",
      "Random Forest - RMSE: 0.3807886552931954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elean\\AppData\\Local\\Temp\\ipykernel_32068\\2322386594.py:47: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  final_predictions = mode(predictions, axis=1)[0].flatten()  # Utilise le vote majoritaire pour obtenir les prédictions finales\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Définition de la classe RandomForestClassifier\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2, max_features=None):\n",
    "        self.n_estimators = n_estimators  # Nombre d'arbres dans la forêt\n",
    "        self.max_depth = max_depth  # Profondeur maximale des arbres\n",
    "        self.min_samples_split = min_samples_split  # Nombre minimum d'échantillons requis pour diviser un nœud\n",
    "        self.max_features = max_features  # Nombre de caractéristiques à considérer pour chaque division\n",
    "        self.trees = []  # Liste pour stocker les arbres\n",
    "        self.selected_features = []  # Liste pour stocker les indices des caractéristiques sélectionnées pour chaque arbre\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape  # Nombre d'échantillons et de caractéristiques\n",
    "        if not self.max_features:\n",
    "            self.max_features = int(np.sqrt(n_features))  # Définit max_features à la racine carrée du nombre de caractéristiques si non spécifié\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Bootstrap sampling\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)  # Sélectionne des échantillons aléatoires avec remise\n",
    "            X_bootstrap = X[indices]  # Échantillon bootstrap pour les caractéristiques\n",
    "            y_bootstrap = y[indices]  # Échantillon bootstrap pour les étiquettes\n",
    "\n",
    "            # Sélection aléatoire des caractéristiques\n",
    "            selected_features = np.random.choice(n_features, self.max_features, replace=False)  # Sélectionne aléatoirement des caractéristiques\n",
    "            self.selected_features.append(selected_features)  # Sauvegarde les indices des caractéristiques sélectionnées\n",
    "\n",
    "            # Création et entraînement de l'arbre de décision\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth, min_samples_split=self.min_samples_split)  # Crée un arbre de décision\n",
    "            tree.fit(X_bootstrap[:, selected_features], y_bootstrap)  # Entraîne l'arbre sur l'échantillon bootstrap et les caractéristiques sélectionnées\n",
    "\n",
    "            self.trees.append(tree)  # Ajoute l'arbre à la forêt\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Agréger les prédictions de chaque arbre\n",
    "        predictions = np.zeros((X.shape[0], len(self.trees)))  # Crée un tableau pour stocker les prédictions de chaque arbre pour chaque échantillon\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            selected_features = self.selected_features[i]  # Récupère les caractéristiques sélectionnées pour cet arbre\n",
    "            predictions[:, i] = tree.predict(X[:, selected_features])  # Prédictions de l'arbre en utilisant les caractéristiques sélectionnées\n",
    "        \n",
    "        # Vote majoritaire\n",
    "        final_predictions = mode(predictions, axis=1)[0].flatten()  # Utilise le vote majoritaire pour obtenir les prédictions finales\n",
    "        return final_predictions.astype(int)  # Retourne les prédictions finales en tant qu'entiers\n",
    "\n",
    "# Exemple d'utilisation\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)  # Génère un ensemble de données de classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Divise les données en ensembles d'entraînement et de test\n",
    "\n",
    "# Création du modèle de Random Forest\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2, max_features=None)  # Crée une instance de RandomForestClassifier\n",
    "\n",
    "# Entraînement du modèle sur les données d'entraînement\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions sur les données de test\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Évaluation du modèle\n",
    "mse_rf = mean_squared_error(y_test, y_pred)  # Calcul de l'erreur quadratique moyenne\n",
    "r2_rf = r2_score(y_test, y_pred)  # Calcul du coefficient de détermination\n",
    "rmse_rf = np.sqrt(mse_rf)  # Calcul de la racine de l'erreur quadratique moyenne\n",
    "\n",
    "print(f\"Random Forest - Mean Squared Error: {mse_rf}\")\n",
    "print(f\"Random Forest - R^2 Score: {r2_rf}\")\n",
    "print(f\"Random Forest - RMSE: {rmse_rf}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
